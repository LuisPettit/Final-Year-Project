{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset and partion features into defined feature subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('fyp_dataset.csv')\n",
    "\n",
    "\n",
    "account = pd.DataFrame(data = data[['location','account_age','verified','tweets','label']])\n",
    "tweet_construction = pd.DataFrame(data = data[['sentiment_neg','sentiment_neu','sentiment_pos', 'lexical_diversity','label']])\n",
    "activity = pd.DataFrame(data = data[['following','followers','likes_given','likes_received','label']])\n",
    "interactiveness = pd.DataFrame(data = data[['hashtag_count','mentions_count','url_count','retweets_count','label']])\n",
    "tweet_source = pd.DataFrame(data = data[['Twitter for iPhone','Twitter for Andriod','Twitter for iPad','Twitter for Web Client','Twitter for Websites','Twitter for Web App','Other','label']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use gridsearch to select optimal classifier for each feature subset. Considers Random Forest, Logestic Regression, Decision Tree and kNearest Neighbour as potential classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "frames = [account,tweet_construction,activity,interactiveness,tweet_source]\n",
    "\n",
    "\n",
    "counter = 1\n",
    "\n",
    "for frame in frames:\n",
    "    \n",
    "    y = frame.pop('label').values\n",
    "    X = frame.values\n",
    "\n",
    "    #split data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y)\n",
    "\n",
    "    \n",
    "  \n",
    "    n_neighbors = [int(x) for x in np.linspace(start = 20, stop = 50, num = 2)]\n",
    "    leaf_size = [int(x) for x in np.linspace(start = 20, stop = 50, num = 2)]\n",
    "    weights = ['uniform', 'distance']\n",
    "    algorithm = ['auto', 'ball_tree','kd_tree','brute']\n",
    "\n",
    "    # Create the random grid\n",
    "    random_grid = {'n_neighbors': n_neighbors,\n",
    "               'leaf_size': leaf_size,\n",
    "               'weights': weights,\n",
    "               'algorithm': algorithm}\n",
    "\n",
    "    knn = KNeighborsClassifier()\n",
    "    knn_random = RandomizedSearchCV(estimator = knn, param_distributions = random_grid, n_iter = 10, cv = 5, verbose=2, random_state=42, n_jobs = -1)\n",
    "    # Fit the random search model\n",
    "    knn_random.fit(X_train, y_train)\n",
    "    \n",
    "    #save best model\n",
    "    knn_best = knn_random.best_estimator_\n",
    "    #check best n_estimators value\n",
    "    \n",
    "    print('knn done')\n",
    "    \n",
    "    current_score = knn_best.score(X_test, y_test)\n",
    "    current_classifier = knn_best\n",
    "\n",
    "    \n",
    "    ###### Random Forest ######\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "\n",
    "    # Number of trees in random forest\n",
    "    n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "    # Number of features to consider at every split\n",
    "    max_features = ['auto', 'sqrt']\n",
    "    # Maximum number of levels in tree\n",
    "    max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "    max_depth.append(None)\n",
    "    # Minimum number of samples required to split a node\n",
    "    min_samples_split = [2, 5, 10]\n",
    "    # Minimum number of samples required at each leaf node\n",
    "    min_samples_leaf = [1, 2, 4]\n",
    "    # Method of selecting samples for training each tree\n",
    "    bootstrap = [True, False]\n",
    "\n",
    "    # Create the random grid\n",
    "    random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "\n",
    "\n",
    "    # Use the random grid to search for best hyperparameters\n",
    "    # First create the base model to tune\n",
    "    rf = RandomForestClassifier()\n",
    "    # Random search of parameters, using 3 fold cross validation, \n",
    "    # search across 100 different combinations, and use all available cores\n",
    "    rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 10, cv = 5, verbose=2, random_state=42, n_jobs = -1)\n",
    "    # Fit the random search model\n",
    "    rf_random.fit(X_train, y_train)\n",
    "    \n",
    "    #save best model\n",
    "    rf_best = rf_random.best_estimator_\n",
    "    #check best n_estimators value\n",
    "    \n",
    "    print('Random Forest done')\n",
    "    \n",
    "    if rf_best.score(X_test, y_test) > current_score:\n",
    "        current_score = rf_best.score(X_test, y_test)\n",
    "        current_classifier = rf_best\n",
    "\n",
    "\n",
    "    ###### Logistic Regression ######\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    #create a new logistic regression model\n",
    "    log_reg = LogisticRegression(solver='lbfgs')\n",
    "    #fit the model to the training data\n",
    "    #log_reg.fit(X_train, y_train)\n",
    "    #print('LogReg done')\n",
    "\n",
    "    C = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "\n",
    "    \n",
    "    # Create the random grid\n",
    "    random_grid = {'C': C}\n",
    "    \n",
    " \n",
    "    # Random search of parameters, using 3 fold cross validation, \n",
    "    # search across 100 different combinations, and use all available cores\n",
    "    log_reg_random = RandomizedSearchCV(estimator = log_reg, param_distributions = random_grid, n_iter = 10, cv = 5, verbose=2, random_state=42, n_jobs = -1)\n",
    "    # Fit the random search model\n",
    "    log_reg_random.fit(X_train, y_train)\n",
    "    \n",
    "    #save best model\n",
    "    log_reg_best = log_reg_random.best_estimator_\n",
    "    #check best n_estimators value\n",
    "    \n",
    "    print('Loegistic Regression done')\n",
    "\n",
    "    \n",
    "    if log_reg_best.score(X_test, y_test) > current_score:\n",
    "        current_score = log_reg_best.score(X_test, y_test)\n",
    "        current_classifier = log_reg_best\n",
    "\n",
    "\n",
    "    ###### Decision Tree ######\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    #naive_b = DecisionTreeClassifier()\n",
    "    #naive_b.fit(X_train, y_train)\n",
    "    \n",
    "    criterion = ['gini', 'entropy']\n",
    "    max_features = ['auto', 'sqrt', 'log2']\n",
    "    min_samples_split = [int(x) for x in np.linspace(2, 110, num = 20)]\n",
    "    min_samples_leaf = [int(x) for x in np.linspace(2, 110, num = 20)]\n",
    "\n",
    "    \n",
    "    # Create the random grid\n",
    "    random_grid = {'criterion': criterion,\n",
    "               'max_features': max_features,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf}\n",
    "    \n",
    "    \n",
    "    d_tree = DecisionTreeClassifier()\n",
    "    # Random search of parameters, using 3 fold cross validation, \n",
    "    # search across 100 different combinations, and use all available cores\n",
    "    d_tree_random = RandomizedSearchCV(estimator = d_tree, param_distributions = random_grid, n_iter = 10, cv = 5, verbose=2, random_state=42, n_jobs = -1)\n",
    "    # Fit the random search model\n",
    "    d_tree_random.fit(X_train, y_train)\n",
    "    \n",
    "    #save best model\n",
    "    d_tree_best = d_tree_random.best_estimator_\n",
    "    #check best n_estimators value\n",
    "    \n",
    "    print('Decision tree done')\n",
    "    \n",
    "    if d_tree_best.score(X_test, y_test) > current_score:\n",
    "        current_score = d_tree_best.score(X_test, y_test)\n",
    "        current_classifier = d_tree_best\n",
    "    \n",
    "      \n",
    "    if counter == 1:\n",
    "        account_classifier = current_classifier\n",
    "        account_score = current_score\n",
    "    elif counter == 2:\n",
    "        #sentiment_classifier = current_classifier\n",
    "        #sentiment_score = current_score\n",
    "        tweet_construction_classifier = current_classifier\n",
    "        tweet_construction_score = current_score\n",
    "    elif counter == 3:\n",
    "        activity_classifier = current_classifier\n",
    "        activity_score = current_score\n",
    "        #interactiveness_classifier = current_classifier\n",
    "        #interactiveness_score = current_score\n",
    "    elif counter == 4:\n",
    "        interactiveness_classifier = current_classifier\n",
    "        interactiveness_score = current_score\n",
    "    else:\n",
    "        tweet_source_classifier = current_classifier\n",
    "        tweet_source_score = current_score\n",
    "        \n",
    "    \n",
    "    print(\"Iteration %d complete\" % counter)\n",
    "    \n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output best performing classifiers for each feature subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "account_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactiveness_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_construction_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_source_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(account_score)\n",
    "   \n",
    "print(sentiment_score)\n",
    "       \n",
    "print(activity_score)\n",
    "   \n",
    "print(interactiveness_score)\n",
    "    \n",
    "print(tweet_source_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section that uses feature subset classifiers to create majority voting classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "data = pd.read_csv('fyp_dataset.csv')\n",
    "\n",
    "data.pop('id').values\n",
    "\n",
    "y = data.pop('label').values\n",
    "X = data.values\n",
    "\n",
    "#split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y)\n",
    "\n",
    "account_classifier = joblib.load(\"joblib//account_classifier.sav\")\n",
    "sentiment_classifier = joblib.load(\"joblib//sentiment_classifier.sav\")\n",
    "activity_classifier = joblib.load(\"joblib//activity_classifier.sav\")\n",
    "interactiveness_classifier = joblib.load(\"joblib//interactiveness_classifier.sav\")\n",
    "tweet_source_classifier = joblib.load(\"joblib//tweet_source_classifier1.sav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "#create a dictionary of our models\n",
    "estimators=[('account', account_classifier), ('sentiment', tweet_construction_classifier), ('activity', activity_classifier), ('interactiveness', interactiveness_classifier), ('source', tweet_source_classifier)]\n",
    "#create our voting classifier, inputting our models\n",
    "ensemble_updated = VotingClassifier(estimators, voting='soft', weights=[0.149, 0.095, 0.311, 0.275, 0.17]) #all\n",
    "#ensemble_updated = VotingClassifier(estimators, voting='soft', weights=[0.158, 0.036, 0.332, 0.293, 0.181])# without lex\n",
    "\n",
    "#ensemble_updated = VotingClassifier(estimators, voting='soft', weights=[0.192, 0.044, 0.403, 0.361])# without lex and tweet sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit model to training data\n",
    "ensemble_updated.fit(X_train, y_train)\n",
    "#test our model on the test data\n",
    "ensemble_updated.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to disk\n",
    "filename = 'joblib//ensemble_classifier101.sav'\n",
    "joblib.dump(ensemble_updated, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "\n",
    "scoring = {'accuracy': 'accuracy',\n",
    "           'recall': 'recall',\n",
    "           'precision': 'precision',\n",
    "           'roc_auc': 'roc_auc',\n",
    "           'f1' : 'f1'}\n",
    "\n",
    "#frame = pd.read_csv('fyp_dataset.csv')\n",
    "\n",
    "data = pd.read_csv('fyp_dataset.csv')\n",
    "\n",
    "data.pop('id').values\n",
    "\n",
    "y = data.pop('label').values\n",
    "X = data.values\n",
    "\n",
    "\n",
    "cross_val_scores = cross_validate(ensemble_updated, X, y, cv=5, scoring = scoring)\n",
    "cross_val_scores\n",
    "\n",
    "\n",
    "reports1 = []\n",
    "\n",
    "for i in range(5):\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size=1/5)\n",
    "\n",
    "\n",
    "    fitted_model = ensemble_updated.fit(X_train, y_train)\n",
    "    y_dash = fitted_model.predict(X_test)\n",
    "    \n",
    "    reports1.append(classification_report(y_test, y_dash, target_names = ['Human','Bot']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cross_val_scores['test_accuracy'].mean())\n",
    "print(cross_val_scores['test_recall'].mean())\n",
    "print(cross_val_scores['test_precision'].mean())\n",
    "print(cross_val_scores['test_roc_auc'].mean())\n",
    "print(cross_val_scores['test_f1'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for report in reports1:\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save trained classifiers locally as pickle files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('pickle//with lex div//account_classifier.pkl', 'wb') as fid:\n",
    "    pickle.dump(account_classifier, fid) \n",
    "fid.close()\n",
    "\n",
    "with open('pickle//with lex div//sentiment_classifier.pkl', 'wb') as fid:\n",
    "    pickle.dump(tweet_construction_classifier, fid) \n",
    "fid.close()\n",
    "\n",
    "with open('pickle//with lex div//activity_classifier.pkl', 'wb') as fid:\n",
    "    pickle.dump(activity_classifier, fid) \n",
    "fid.close()\n",
    "\n",
    "with open('pickle//with lex div//interactiveness_classifier.pkl', 'wb') as fid:\n",
    "    pickle.dump(interactiveness_classifier, fid) \n",
    "fid.close()\n",
    "\n",
    "with open('pickle//with lex div//tweet_source_classifier.pkl', 'wb') as fid:\n",
    "    pickle.dump(tweet_source_classifier, fid) \n",
    "fid.close()\n",
    "\n",
    "with open('pickle//with lex div//ensemble_classifier.pkl', 'wb') as fid:\n",
    "    pickle.dump(ensemble_updated, fid) \n",
    "fid.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save trained classifiers locally as joblib files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# save the model to disk\n",
    "filename = 'joblib//account_classifier.sav'\n",
    "joblib.dump(account_classifier, filename)\n",
    "\n",
    "# save the model to disk\n",
    "filename = 'joblib//sentiment_classifier.sav'\n",
    "joblib.dump(tweet_construction_classifier, filename)\n",
    "\n",
    "# save the model to disk\n",
    "filename = 'joblib//activity_classifier.sav'\n",
    "joblib.dump(activity_classifier, filename)\n",
    "\n",
    "# save the model to disk\n",
    "filename = 'joblib//interactiveness_classifier.sav'\n",
    "joblib.dump(interactiveness_classifier, filename)\n",
    "\n",
    "# save the model to disk\n",
    "filename = 'joblib//tweet_source_classifier.sav'\n",
    "joblib.dump(tweet_source_classifier, filename)\n",
    "\n",
    "# save the model to disk\n",
    "filename = 'joblib//ensemble_classifier.sav'\n",
    "joblib.dump(ensemble_updated, filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
